{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cf45e7-481a-4741-a4f1-81ce9bea5d8e",
   "metadata": {},
   "source": [
    "# ðŸŽ² Toy Problem: \"Two Actions\"\n",
    "\n",
    "Imagine the agent is in an environment with two possible actions:\n",
    "\n",
    "- Action 0 â†’ Gives a reward of +1 with 30% probability, otherwise 0.\n",
    "- Action 1 â†’ Gives a reward of +1 with 70% probability, otherwise 0.\n",
    "\n",
    "At the beginning, the agent doesnâ€™t know this. It must interact with the environment and learn by trial and error which action gives higher reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c118f6-2a9e-4df0-8ebf-02cc048cb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793abea7-67f7-4fd8-8321-eede3850ecf6",
   "metadata": {},
   "source": [
    "## Simple environment with two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf17a52b-0dbf-4e97-8dbb-703a65169430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(action):\n",
    "    if action == 0:\n",
    "        return 1 if np.random.rand() < 0.3 else 0\n",
    "    elif action == 1:\n",
    "        return 1 if np.random.rand() < 0.7 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c1d93-0629-404f-a2c2-17ce01641be0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c886164e-a230-4066-ad59-9f17f8c1bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "estimated_values = [0.0, 0.0] # estimated average value for each action\n",
    "counts = [0, 0]               # how many times each action was chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5ffe5-db53-476f-bd28-74453e0fbacc",
   "metadata": {},
   "source": [
    "## Strategy: Îµ-greedy (sometimes explore, mostly exploit what seems best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5ff9ef-2e94-47a4-816c-175c25fd782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1 | Action: 0 | Reward: 0 | Estimated values: [0.0, 0.0]\n",
      "Episode   2 | Action: 0 | Reward: 1 | Estimated values: [0.5, 0.0]\n",
      "Episode   3 | Action: 0 | Reward: 0 | Estimated values: [0.33333333333333337, 0.0]\n",
      "Episode   4 | Action: 0 | Reward: 0 | Estimated values: [0.25, 0.0]\n",
      "Episode   5 | Action: 0 | Reward: 1 | Estimated values: [0.4, 0.0]\n",
      "Episode   6 | Action: 0 | Reward: 0 | Estimated values: [0.33333333333333337, 0.0]\n",
      "Episode   7 | Action: 0 | Reward: 0 | Estimated values: [0.28571428571428575, 0.0]\n",
      "Episode   8 | Action: 0 | Reward: 0 | Estimated values: [0.25000000000000006, 0.0]\n",
      "Episode   9 | Action: 0 | Reward: 0 | Estimated values: [0.22222222222222227, 0.0]\n",
      "Episode  10 | Action: 0 | Reward: 1 | Estimated values: [0.30000000000000004, 0.0]\n",
      "Episode  11 | Action: 0 | Reward: 0 | Estimated values: [0.27272727272727276, 0.0]\n",
      "Episode  12 | Action: 0 | Reward: 0 | Estimated values: [0.25000000000000006, 0.0]\n",
      "Episode  13 | Action: 0 | Reward: 1 | Estimated values: [0.30769230769230776, 0.0]\n",
      "Episode  14 | Action: 0 | Reward: 0 | Estimated values: [0.2857142857142858, 0.0]\n",
      "Episode  15 | Action: 0 | Reward: 0 | Estimated values: [0.2666666666666668, 0.0]\n",
      "Episode  16 | Action: 0 | Reward: 0 | Estimated values: [0.2500000000000001, 0.0]\n",
      "Episode  17 | Action: 0 | Reward: 1 | Estimated values: [0.29411764705882365, 0.0]\n",
      "Episode  18 | Action: 0 | Reward: 0 | Estimated values: [0.2777777777777779, 0.0]\n",
      "Episode  19 | Action: 0 | Reward: 0 | Estimated values: [0.2631578947368422, 0.0]\n",
      "Episode  20 | Action: 0 | Reward: 1 | Estimated values: [0.3000000000000001, 0.0]\n",
      "Episode  21 | Action: 0 | Reward: 0 | Estimated values: [0.2857142857142858, 0.0]\n",
      "Episode  22 | Action: 0 | Reward: 1 | Estimated values: [0.3181818181818183, 0.0]\n",
      "Episode  23 | Action: 0 | Reward: 0 | Estimated values: [0.3043478260869566, 0.0]\n",
      "Episode  24 | Action: 1 | Reward: 0 | Estimated values: [0.3043478260869566, 0.0]\n",
      "Episode  25 | Action: 0 | Reward: 0 | Estimated values: [0.29166666666666674, 0.0]\n",
      "Episode  26 | Action: 0 | Reward: 0 | Estimated values: [0.2800000000000001, 0.0]\n",
      "Episode  27 | Action: 0 | Reward: 0 | Estimated values: [0.26923076923076933, 0.0]\n",
      "Episode  28 | Action: 0 | Reward: 1 | Estimated values: [0.2962962962962964, 0.0]\n",
      "Episode  29 | Action: 0 | Reward: 1 | Estimated values: [0.3214285714285715, 0.0]\n",
      "Episode  30 | Action: 0 | Reward: 1 | Estimated values: [0.34482758620689663, 0.0]\n",
      "Episode  31 | Action: 0 | Reward: 0 | Estimated values: [0.3333333333333334, 0.0]\n",
      "Episode  32 | Action: 0 | Reward: 0 | Estimated values: [0.3225806451612904, 0.0]\n",
      "Episode  33 | Action: 0 | Reward: 0 | Estimated values: [0.3125000000000001, 0.0]\n",
      "Episode  34 | Action: 0 | Reward: 0 | Estimated values: [0.30303030303030315, 0.0]\n",
      "Episode  35 | Action: 1 | Reward: 0 | Estimated values: [0.30303030303030315, 0.0]\n",
      "Episode  36 | Action: 0 | Reward: 1 | Estimated values: [0.323529411764706, 0.0]\n",
      "Episode  37 | Action: 0 | Reward: 0 | Estimated values: [0.3142857142857144, 0.0]\n",
      "Episode  38 | Action: 0 | Reward: 0 | Estimated values: [0.30555555555555564, 0.0]\n",
      "Episode  39 | Action: 0 | Reward: 1 | Estimated values: [0.3243243243243244, 0.0]\n",
      "Episode  40 | Action: 0 | Reward: 1 | Estimated values: [0.3421052631578948, 0.0]\n",
      "Episode  41 | Action: 0 | Reward: 0 | Estimated values: [0.33333333333333337, 0.0]\n",
      "Episode  42 | Action: 0 | Reward: 0 | Estimated values: [0.325, 0.0]\n",
      "Episode  43 | Action: 0 | Reward: 0 | Estimated values: [0.3170731707317073, 0.0]\n",
      "Episode  44 | Action: 0 | Reward: 0 | Estimated values: [0.30952380952380953, 0.0]\n",
      "Episode  45 | Action: 0 | Reward: 0 | Estimated values: [0.3023255813953489, 0.0]\n",
      "Episode  46 | Action: 0 | Reward: 0 | Estimated values: [0.29545454545454547, 0.0]\n",
      "Episode  47 | Action: 0 | Reward: 0 | Estimated values: [0.2888888888888889, 0.0]\n",
      "Episode  48 | Action: 0 | Reward: 0 | Estimated values: [0.28260869565217395, 0.0]\n",
      "Episode  49 | Action: 0 | Reward: 0 | Estimated values: [0.2765957446808511, 0.0]\n",
      "Episode  50 | Action: 0 | Reward: 0 | Estimated values: [0.27083333333333337, 0.0]\n",
      "Episode  51 | Action: 0 | Reward: 0 | Estimated values: [0.2653061224489796, 0.0]\n",
      "Episode  52 | Action: 0 | Reward: 0 | Estimated values: [0.26, 0.0]\n",
      "Episode  53 | Action: 0 | Reward: 0 | Estimated values: [0.2549019607843137, 0.0]\n",
      "Episode  54 | Action: 0 | Reward: 0 | Estimated values: [0.24999999999999997, 0.0]\n",
      "Episode  55 | Action: 0 | Reward: 1 | Estimated values: [0.2641509433962264, 0.0]\n",
      "Episode  56 | Action: 0 | Reward: 0 | Estimated values: [0.25925925925925924, 0.0]\n",
      "Episode  57 | Action: 0 | Reward: 0 | Estimated values: [0.2545454545454545, 0.0]\n",
      "Episode  58 | Action: 0 | Reward: 0 | Estimated values: [0.24999999999999997, 0.0]\n",
      "Episode  59 | Action: 0 | Reward: 0 | Estimated values: [0.24561403508771928, 0.0]\n",
      "Episode  60 | Action: 0 | Reward: 0 | Estimated values: [0.24137931034482757, 0.0]\n",
      "Episode  61 | Action: 0 | Reward: 0 | Estimated values: [0.23728813559322032, 0.0]\n",
      "Episode  62 | Action: 0 | Reward: 0 | Estimated values: [0.2333333333333333, 0.0]\n",
      "Episode  63 | Action: 0 | Reward: 1 | Estimated values: [0.24590163934426226, 0.0]\n",
      "Episode  64 | Action: 0 | Reward: 1 | Estimated values: [0.25806451612903225, 0.0]\n",
      "Episode  65 | Action: 0 | Reward: 0 | Estimated values: [0.25396825396825395, 0.0]\n",
      "Episode  66 | Action: 0 | Reward: 1 | Estimated values: [0.265625, 0.0]\n",
      "Episode  67 | Action: 0 | Reward: 0 | Estimated values: [0.26153846153846155, 0.0]\n",
      "Episode  68 | Action: 0 | Reward: 1 | Estimated values: [0.27272727272727276, 0.0]\n",
      "Episode  69 | Action: 0 | Reward: 0 | Estimated values: [0.2686567164179105, 0.0]\n",
      "Episode  70 | Action: 0 | Reward: 0 | Estimated values: [0.26470588235294124, 0.0]\n",
      "Episode  71 | Action: 0 | Reward: 1 | Estimated values: [0.27536231884057977, 0.0]\n",
      "Episode  72 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.3333333333333333]\n",
      "Episode  73 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.5]\n",
      "Episode  74 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6]\n",
      "Episode  75 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6666666666666666]\n",
      "Episode  76 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.7142857142857143]\n",
      "Episode  77 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.75]\n",
      "Episode  78 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.7777777777777778]\n",
      "Episode  79 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.8]\n",
      "Episode  80 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.8181818181818182]\n",
      "Episode  81 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.75]\n",
      "Episode  82 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.6923076923076923]\n",
      "Episode  83 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.6428571428571428]\n",
      "Episode  84 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6666666666666666]\n",
      "Episode  85 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.625]\n",
      "Episode  86 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.5882352941176471]\n",
      "Episode  87 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.5555555555555556]\n",
      "Episode  88 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.5789473684210527]\n",
      "Episode  89 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6]\n",
      "Episode  90 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6190476190476191]\n",
      "Episode  91 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6363636363636364]\n",
      "Episode  92 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6521739130434783]\n",
      "Episode  93 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6666666666666666]\n",
      "Episode  94 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6799999999999999]\n",
      "Episode  95 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6923076923076923]\n",
      "Episode  96 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.7037037037037037]\n",
      "Episode  97 | Action: 1 | Reward: 0 | Estimated values: [0.27536231884057977, 0.6785714285714286]\n",
      "Episode  98 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.6896551724137931]\n",
      "Episode  99 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.7000000000000001]\n",
      "Episode 100 | Action: 1 | Reward: 1 | Estimated values: [0.27536231884057977, 0.7096774193548387]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "for t in range(1, n_episodes + 1):\n",
    "    # choose action (exploration vs. exploitation)\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice([0, 1])    # explore\n",
    "    else:\n",
    "        action = np.argmax(estimated_values) # exploit\n",
    "\n",
    "    # Execute action in the environment\n",
    "    reward = environment(action)\n",
    "\n",
    "    # update counts and estimated value of the action\n",
    "    counts[action] += 1\n",
    "    estimated_values[action] += (reward - estimated_values[action]) / counts[action]\n",
    "\n",
    "    print(f\"Episode {t:3d} | Action: {action} | Reward: {reward} | Estimated values: {estimated_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
